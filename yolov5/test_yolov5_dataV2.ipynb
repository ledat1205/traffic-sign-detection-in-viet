{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_yolov5_dataV2.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tmg4I2NzpYui","executionInfo":{"status":"ok","timestamp":1654106342607,"user_tz":-420,"elapsed":14119,"user":{"displayName":"Anh VÅ© Tuáº¥n","userId":"02618545363164433519"}},"outputId":"9fd7f88f-4d23-4943-83bf-3630e87a03b8"},"outputs":[{"output_type":"stream","name":"stderr","text":["YOLOv5 ðŸš€ v6.1-243-g7cef03d Python-3.7.13 torch-1.11.0+cu113 CPU\n"]},{"output_type":"stream","name":"stdout","text":["Setup complete âœ… (2 CPUs, 12.7 GB RAM, 38.4/107.7 GB disk)\n"]}],"source":["!git clone https://github.com/ultralytics/yolov5  # clone\n","%cd yolov5\n","%pip install -qr requirements.txt  # install\n","\n","import torch\n","import utils\n","display = utils.notebook_init()  # checks"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WsmGiwm2pgKo","executionInfo":{"status":"ok","timestamp":1654106363423,"user_tz":-420,"elapsed":20823,"user":{"displayName":"Anh VÅ© Tuáº¥n","userId":"02618545363164433519"}},"outputId":"d4e23c36-e17a-45a5-9458-60176e3ac464"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!python val.py --weights /content/drive/MyDrive/CV_nangcao/train/yolov5/yolov5_result_dataV2/exp3/weights/best.pt --data /content/drive/MyDrive/CV_nangcao/train/yolov5/dataTest.yaml --img 640 --iou 0.65 --half --project /content/drive/MyDrive/CV_nangcao/train/yolov5/yolov5_result_dataV2/val"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YsV8g9rWpiAf","executionInfo":{"status":"ok","timestamp":1654106664695,"user_tz":-420,"elapsed":301284,"user":{"displayName":"Anh VÅ© Tuáº¥n","userId":"02618545363164433519"}},"outputId":"b720ad94-0628-42d5-90ea-b689cc1479af"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=/content/drive/MyDrive/CV_nangcao/train/yolov5/dataTest.yaml, weights=['/content/drive/MyDrive/CV_nangcao/train/yolov5/yolov5_result_dataV2/exp3/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=/content/drive/MyDrive/CV_nangcao/train/yolov5/yolov5_result_dataV2/val, name=exp, exist_ok=False, half=True, dnn=False\n","YOLOv5 ðŸš€ v6.1-243-g7cef03d Python-3.7.13 torch-1.11.0+cu113 CPU\n","\n","Fusing layers... \n","Model summary: 290 layers, 20873139 parameters, 0 gradients, 48.0 GFLOPs\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 18.3MB/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/CV_nangcao/data/data_2_version/dataTest/labels/test' images and labels...277 found, 0 missing, 0 empty, 0 corrupt: 100% 277/277 [01:28<00:00,  3.11it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/CV_nangcao/data/data_2_version/dataTest/labels/test.cache\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [03:10<00:00, 21.22s/it]\n","                 all        277        532      0.951      0.944      0.968      0.679\n","  Cam di nguoc chieu        277        104       0.96      0.933      0.984       0.61\n","Huong di phai re phai        277          7       0.96          1      0.995      0.784\n","Giao nhau voi duong khong uu tien        277         41      0.973      0.863      0.912      0.613\n","         Nguoi di bo        277         85      0.965       0.97      0.992        0.7\n","           Cam do xe        277         23      0.949          1      0.993      0.725\n","           Bien khac        277        272      0.899      0.901      0.931      0.641\n","Speed: 3.3ms pre-process, 678.2ms inference, 0.8ms NMS per image at shape (32, 3, 640, 640)\n","Results saved to \u001b[1m/content/drive/MyDrive/CV_nangcao/train/yolov5/yolov5_result_dataV2/val/exp2\u001b[0m\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"5sYmnKJbr5V5"},"execution_count":null,"outputs":[]}]}